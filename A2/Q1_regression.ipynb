{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SOUMYADEEP BANIK***<br>\n",
    "***CS-2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import house_sales_data as hd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sacramento house price data; load\n",
    "data = hd.house_sales_data() \n",
    "#print(data.price())\n",
    "\n",
    "## Get the target (i.e., price) and two features\n",
    "# price = ...\n",
    "# area = ...\n",
    "# beds = ...\n",
    "price = data[\"price\"]\n",
    "beds = data[\"beds\"]\n",
    "area = data[\"area\"]\n",
    "df_house_data = pd.DataFrame({\"area1\":area, \"price1\":price}, columns=[\"area1\", \"price1\"])\n",
    "x = df_house_data[\"area1\"].values\n",
    "y = df_house_data[\"price1\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent (NOT SGD, just simple GD!)\n",
    "def gradient_descent(X, y, max_iteration, learning_rate=1.0):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    costs = np.zeros(max_iteration)\n",
    "    # now complete ...\n",
    "    for t in range(max_iteration):\n",
    "        grad = np.zeros(n)\n",
    "        for i in range(m):\n",
    "            y_hat = X[i] @ theta     #multiplying the X matrix with theta col. vector i.e x[i][j]*theta[j]\n",
    "            costs[t] += 1/m * (y_hat - y[i])**2\n",
    "            for j in range(n):\n",
    "                grad[j] += 1/m * 2 * (y_hat - y[i])*X[i,j]\n",
    "        theta -= learning_rate * grad\n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data \n",
    "def normalize_data(X, y, normalize_cols):\n",
    "    min_X = X[:,normalize_cols].min(axis=0)\n",
    "    max_X = X[:,normalize_cols].max(axis=0)\n",
    "    min_y = y.min()\n",
    "    max_y = y.max()\n",
    "    X[:,normalize_cols] = (X[:,normalize_cols] - min_X) / (max_X - min_X)\n",
    "    y[:] = (y - min_y) / (max_y - min_y)\n",
    "    return min_X, max_X, min_y, max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an evaluation metric for regression\n",
    "def evaluation_metric(y,y_hat):\n",
    "    return np.sqrt(np.mean(np.square(y-y_hat)))   #RMSE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0158a072a74a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "# Prepare data this way\n",
    "X_train = np.hstack((area[:,np.newaxis], beds[:,np.newaxis], np.ones(area.shape[0])[:,np.newaxis]))\n",
    "y_train = price\n",
    "X, y, min_X, max_X, min_y, max_y = normalize_data(X_train, y_train, [True, True, False])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call gradient descent and fine-tune your learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost vs iterations \n",
    "# note gradient descent return costs for all iterations\n",
    "def gradient_descent_cost(iters):\n",
    "    x_nor = (x - min(x)) / (max(x) - min(x))\n",
    "    y_nor = (y - min(y)) / (max(y) - min(y))\n",
    "    cost = []\n",
    "    theta = np.array([0., 0.])\n",
    "    alpha = 1.0\n",
    "    for t in range(iters):\n",
    "        theta[0] = alpha/len(x) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "        theta[1] = alpha/len(x) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor) )\n",
    "        cost.append(np.mean((theta[0] * x_nor + theta[1] - y_nor)**2))\n",
    "        theta[0] -= alpha/len(x) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "        theta[1] -= alpha/len(x) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor) )\n",
    "    return np.array(cost)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0,100), gradient_descent_cost(100))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metric for test set while \n",
    "# changing max_iteration from 500 to 2000\n",
    "# Note the best value of evaluation metric you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare your solution with scikit-learn library\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "order = np.random.permutation(len(x))\n",
    "portion = 20\n",
    "test_x = x[order[:portion]]\n",
    "test_y = y[order[:portion]]\n",
    "train_x = x[order[portion:]]\n",
    "train_y = y[order[portion:]]\n",
    "lin_reg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.315 2.214 0.8   1.505 2.142 1.578 1.08  1.45  1.207 1.473 1.416 1.69\n 3.468 2.724 0.904 1.337 1.968 0.795 1.202 1.58  1.582 1.531 1.89  1.3\n 1.284 2.346 1.188 1.272 1.302 1.462 1.713 1.185 1.058 2.347 3.508 1.69\n 0.975 1.638 1.41  2.734 1.993 0.984 1.089 1.609 2.155 2.187 1.811 2.877\n 3.984 1.348 1.139 1.174 1.672 2.096 3.863 1.627 1.094 1.605 1.302 1.555\n 3.192 1.386 1.022 1.81  1.936 3.433 2.577 2.093 3.173 1.419 1.646 0.81\n 1.472 1.739 1.118 1.249 1.25  1.808 1.112 1.08  1.497 0.81  1.939 1.795\n 1.206 1.685 1.636 1.543 1.67  1.356 0.844 1.137 0.8   1.074 2.457 2.555\n 1.302 1.067 1.01  1.277 1.448 1.32  1.574 2.379 1.83  1.207 1.715 1.566\n 1.843 1.12  1.448 1.603 1.326 1.718 1.104 1.52  1.187 1.324 1.012 1.962\n 1.367 2.258 1.74  1.511 0.796 1.83  1.714 1.543 1.321 1.401 1.285 1.202\n 1.344 1.051 1.093 1.991 1.488 1.65  2.605 1.37  1.011 2.295 0.901 1.229\n 1.59  0.966 1.587 2.494 1.616 0.972 3.397 1.265 2.254 1.15  1.843 1.039\n 1.159 1.064 1.127 2.359 1.007 1.156 2.875 3.229 2.711 0.958 1.424 1.655\n 2.494 1.915 1.12  2.372 1.25  1.522 1.261 3.056 1.953 1.677 0.9   0.96\n 3.261 1.17  1.159 1.477 1.34  3.357 1.724 1.162 1.498 0.682 0.99  2.491\n 1.12  1.8   0.539 1.503 0.756 1.22  1.152 2.66  1.582 0.861 1.306 1.416\n 1.291 1.    1.029 0.876 2.212 1.309 0.804 1.441 1.479 2.28  1.104 1.343\n 2.508 1.52  1.1   1.154 2.96  4.303 1.477 1.478 1.276 1.28  1.216 1.169\n 1.119 1.12  1.502 1.452 0.722 1.161 1.06  3.389 1.905 1.176 1.638 1.828\n 1.451 1.939 2.962 2.354 1.08  1.099 1.567 2.068 1.406 1.41  3.599 1.13\n 1.586 1.092 2.159 0.96  1.446 1.212 1.516 3.788 1.981 1.38  1.57  1.182\n 1.606 1.892 0.96  1.329 1.253 1.351 1.639 0.61  2.606 1.232 1.735 0.97\n 1.596 1.479 3.281 1.172 2.356 1.296 0.779 1.006 1.51  1.16  1.77  1.74\n 1.11  1.341 1.591 1.443 0.868 1.58  1.089 0.988 2.462 1.438 0.95  1.289\n 1.685 0.904 1.258 3.164 1.317 1.08  2.484 1.24  2.367 3.741 1.331 2.169\n 1.235 0.904 1.716 1.32  2.17  0.984 0.911 2.846 3.07  1.876 1.517 2.004\n 1.164 1.529 1.329 2.55  1.982 1.485 1.541 2.126 1.436 1.273 2.28  1.211\n 1.204 3.615 1.904 2.163 1.669 1.264 1.294 3.579 1.872 1.146 1.032 1.24\n 1.43  1.371 0.881 1.463 1.043 1.671 1.376 1.152 2.824 1.475 2.309 3.516\n 1.45  1.174 0.888 1.77  1.524 2.11  1.527 0.941 0.962 2.111 2.222 1.039\n 1.616 2.053 1.721 1.697 1.953 2.724 1.587 0.967 2.334 1.04  0.796 1.098\n 1.316 0.723 2.056 1.418 1.204 1.41  1.082 1.462 1.856 2.185 0.963 1.174\n 1.176 1.092 1.188 1.857 1.712 1.255 1.41  0.932 1.401 1.457 1.032 1.917\n 1.189 3.134 1.152 1.91  1.116 1.418 1.073 1.31  1.266 1.921 1.851 1.37\n 1.199 1.082 1.058 1.955 1.144 1.751 1.338 2.109 0.623 1.362 1.643 1.82\n 1.144 2.687 1.577 0.746 1.375 2.79  1.247 1.341 1.838 1.04  1.857 1.653\n 0.884 2.274 1.638 0.832 1.36  0.958 0.838 2.494 0.918 1.103 1.816 1.54\n 2.787 0.625 1.567 1.269 1.264 0.888 1.915 2.278 1.914 1.305 1.776 1.219\n 2.592 1.02  1.695 1.139 2.1   1.727 1.356 1.393 1.516 1.158 0.906 1.457\n 1.559 0.78  0.936 0.795 0.924 1.876 2.575 2.725 2.235 1.1   1.72  2.503\n 0.964 0.864 1.574 2.581 1.089 1.691 1.    1.266 1.392 2.016 1.104 1.768\n 1.253 1.593 1.448 0.901 1.416 0.846 1.12  1.4   1.304 3.838 1.039 2.325\n 1.595 1.12  1.362 2.447 1.608 2.169 1.638 1.789 1.42  1.056 0.972 0.952\n 1.269 2.109 1.179 1.196 1.659 0.76  1.776 1.65  1.381 2.504 1.265 1.366\n 1.439 1.217 1.358 1.456 1.623 1.112 0.93  3.26  2.166 0.933 1.304 1.088\n 0.924 2.896 1.28  1.127 1.011 1.232 3.746 1.406 1.477 2.16  1.549 1.39\n 1.38  1.258 1.882 1.449 1.108 1.59  1.292 1.132 3.134 1.829 1.248 1.711\n 0.97  1.262 1.248 1.94  1.209 2.329 1.354 1.144 1.394 0.888 1.328 1.656\n 1.411 1.013 1.351 1.548 1.252 0.994 2.056 2.306 1.888 1.428 0.924 1.32\n 1.117 1.188 1.115 1.512 1.319 1.427 0.834 1.758 1.601 1.637 0.722 2.484\n 1.407 2.789 1.598 0.795 1.124 1.075 0.795 1.12  1.483 1.621 1.799 1.051\n 1.177 1.506 1.961 2.054 1.766 1.193 2.367 1.287 1.146 2.175 2.258 1.127\n 1.52  3.992 1.289 1.245 1.269 3.881 1.922 1.08  1.466 1.262 1.436 2.002\n 1.093 0.998 1.14  1.26  1.231 1.801 1.887 1.452 1.628 1.004 1.468 0.99\n 1.358 2.136 2.218 1.44  0.956 1.928 1.12  2.846 2.607 1.134 0.611 1.373\n 1.354 1.36  2.372 1.9   0.696 2.085 1.844 1.851 1.595 1.115 3.037 2.03\n 1.183 1.28  1.678 1.327 2.724 1.45  2.992 1.428 1.364 2.442 0.84  1.353\n 1.216 1.596 1.329 2.8   1.126 1.676 0.795 2.161 0.909 1.715 1.039 3.44\n 1.871 1.899 1.392 1.123 3.44  1.009 1.788 2.199 2.205 4.246 2.052 2.212\n 1.175 1.157 1.676 0.588 2.172 3.076 2.307 1.358 1.039 0.904 1.139 1.382\n 0.836 1.41  1.683 1.465 1.871 1.736 1.328 1.578 1.513 1.144 1.082 1.8\n 0.722 1.8   0.957 2.049 1.686 1.05  1.456 1.59  2.652 1.555].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-d1f728b659cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,\n\u001b[1;32m--> 519\u001b[1;33m                                    y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    819\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\soumyadbanik\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    639\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.315 2.214 0.8   1.505 2.142 1.578 1.08  1.45  1.207 1.473 1.416 1.69\n 3.468 2.724 0.904 1.337 1.968 0.795 1.202 1.58  1.582 1.531 1.89  1.3\n 1.284 2.346 1.188 1.272 1.302 1.462 1.713 1.185 1.058 2.347 3.508 1.69\n 0.975 1.638 1.41  2.734 1.993 0.984 1.089 1.609 2.155 2.187 1.811 2.877\n 3.984 1.348 1.139 1.174 1.672 2.096 3.863 1.627 1.094 1.605 1.302 1.555\n 3.192 1.386 1.022 1.81  1.936 3.433 2.577 2.093 3.173 1.419 1.646 0.81\n 1.472 1.739 1.118 1.249 1.25  1.808 1.112 1.08  1.497 0.81  1.939 1.795\n 1.206 1.685 1.636 1.543 1.67  1.356 0.844 1.137 0.8   1.074 2.457 2.555\n 1.302 1.067 1.01  1.277 1.448 1.32  1.574 2.379 1.83  1.207 1.715 1.566\n 1.843 1.12  1.448 1.603 1.326 1.718 1.104 1.52  1.187 1.324 1.012 1.962\n 1.367 2.258 1.74  1.511 0.796 1.83  1.714 1.543 1.321 1.401 1.285 1.202\n 1.344 1.051 1.093 1.991 1.488 1.65  2.605 1.37  1.011 2.295 0.901 1.229\n 1.59  0.966 1.587 2.494 1.616 0.972 3.397 1.265 2.254 1.15  1.843 1.039\n 1.159 1.064 1.127 2.359 1.007 1.156 2.875 3.229 2.711 0.958 1.424 1.655\n 2.494 1.915 1.12  2.372 1.25  1.522 1.261 3.056 1.953 1.677 0.9   0.96\n 3.261 1.17  1.159 1.477 1.34  3.357 1.724 1.162 1.498 0.682 0.99  2.491\n 1.12  1.8   0.539 1.503 0.756 1.22  1.152 2.66  1.582 0.861 1.306 1.416\n 1.291 1.    1.029 0.876 2.212 1.309 0.804 1.441 1.479 2.28  1.104 1.343\n 2.508 1.52  1.1   1.154 2.96  4.303 1.477 1.478 1.276 1.28  1.216 1.169\n 1.119 1.12  1.502 1.452 0.722 1.161 1.06  3.389 1.905 1.176 1.638 1.828\n 1.451 1.939 2.962 2.354 1.08  1.099 1.567 2.068 1.406 1.41  3.599 1.13\n 1.586 1.092 2.159 0.96  1.446 1.212 1.516 3.788 1.981 1.38  1.57  1.182\n 1.606 1.892 0.96  1.329 1.253 1.351 1.639 0.61  2.606 1.232 1.735 0.97\n 1.596 1.479 3.281 1.172 2.356 1.296 0.779 1.006 1.51  1.16  1.77  1.74\n 1.11  1.341 1.591 1.443 0.868 1.58  1.089 0.988 2.462 1.438 0.95  1.289\n 1.685 0.904 1.258 3.164 1.317 1.08  2.484 1.24  2.367 3.741 1.331 2.169\n 1.235 0.904 1.716 1.32  2.17  0.984 0.911 2.846 3.07  1.876 1.517 2.004\n 1.164 1.529 1.329 2.55  1.982 1.485 1.541 2.126 1.436 1.273 2.28  1.211\n 1.204 3.615 1.904 2.163 1.669 1.264 1.294 3.579 1.872 1.146 1.032 1.24\n 1.43  1.371 0.881 1.463 1.043 1.671 1.376 1.152 2.824 1.475 2.309 3.516\n 1.45  1.174 0.888 1.77  1.524 2.11  1.527 0.941 0.962 2.111 2.222 1.039\n 1.616 2.053 1.721 1.697 1.953 2.724 1.587 0.967 2.334 1.04  0.796 1.098\n 1.316 0.723 2.056 1.418 1.204 1.41  1.082 1.462 1.856 2.185 0.963 1.174\n 1.176 1.092 1.188 1.857 1.712 1.255 1.41  0.932 1.401 1.457 1.032 1.917\n 1.189 3.134 1.152 1.91  1.116 1.418 1.073 1.31  1.266 1.921 1.851 1.37\n 1.199 1.082 1.058 1.955 1.144 1.751 1.338 2.109 0.623 1.362 1.643 1.82\n 1.144 2.687 1.577 0.746 1.375 2.79  1.247 1.341 1.838 1.04  1.857 1.653\n 0.884 2.274 1.638 0.832 1.36  0.958 0.838 2.494 0.918 1.103 1.816 1.54\n 2.787 0.625 1.567 1.269 1.264 0.888 1.915 2.278 1.914 1.305 1.776 1.219\n 2.592 1.02  1.695 1.139 2.1   1.727 1.356 1.393 1.516 1.158 0.906 1.457\n 1.559 0.78  0.936 0.795 0.924 1.876 2.575 2.725 2.235 1.1   1.72  2.503\n 0.964 0.864 1.574 2.581 1.089 1.691 1.    1.266 1.392 2.016 1.104 1.768\n 1.253 1.593 1.448 0.901 1.416 0.846 1.12  1.4   1.304 3.838 1.039 2.325\n 1.595 1.12  1.362 2.447 1.608 2.169 1.638 1.789 1.42  1.056 0.972 0.952\n 1.269 2.109 1.179 1.196 1.659 0.76  1.776 1.65  1.381 2.504 1.265 1.366\n 1.439 1.217 1.358 1.456 1.623 1.112 0.93  3.26  2.166 0.933 1.304 1.088\n 0.924 2.896 1.28  1.127 1.011 1.232 3.746 1.406 1.477 2.16  1.549 1.39\n 1.38  1.258 1.882 1.449 1.108 1.59  1.292 1.132 3.134 1.829 1.248 1.711\n 0.97  1.262 1.248 1.94  1.209 2.329 1.354 1.144 1.394 0.888 1.328 1.656\n 1.411 1.013 1.351 1.548 1.252 0.994 2.056 2.306 1.888 1.428 0.924 1.32\n 1.117 1.188 1.115 1.512 1.319 1.427 0.834 1.758 1.601 1.637 0.722 2.484\n 1.407 2.789 1.598 0.795 1.124 1.075 0.795 1.12  1.483 1.621 1.799 1.051\n 1.177 1.506 1.961 2.054 1.766 1.193 2.367 1.287 1.146 2.175 2.258 1.127\n 1.52  3.992 1.289 1.245 1.269 3.881 1.922 1.08  1.466 1.262 1.436 2.002\n 1.093 0.998 1.14  1.26  1.231 1.801 1.887 1.452 1.628 1.004 1.468 0.99\n 1.358 2.136 2.218 1.44  0.956 1.928 1.12  2.846 2.607 1.134 0.611 1.373\n 1.354 1.36  2.372 1.9   0.696 2.085 1.844 1.851 1.595 1.115 3.037 2.03\n 1.183 1.28  1.678 1.327 2.724 1.45  2.992 1.428 1.364 2.442 0.84  1.353\n 1.216 1.596 1.329 2.8   1.126 1.676 0.795 2.161 0.909 1.715 1.039 3.44\n 1.871 1.899 1.392 1.123 3.44  1.009 1.788 2.199 2.205 4.246 2.052 2.212\n 1.175 1.157 1.676 0.588 2.172 3.076 2.307 1.358 1.039 0.904 1.139 1.382\n 0.836 1.41  1.683 1.465 1.871 1.736 1.328 1.578 1.513 1.144 1.082 1.8\n 0.722 1.8   0.957 2.049 1.686 1.05  1.456 1.59  2.652 1.555].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Train the model using the training sets\n",
    "lin_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set and print evaluation metric\n",
    "# How much does it differ by from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
